# 6.3 Model Propagation

## Model Propagation from Training Pipelines to MCenter Server
Each run of a training pipeline generates a newly trained model. When the pipeline
completes, the model is transferred from the analytical engine to MCenter server,
where it is cataloged into the Governance database along with metadata and provenance
information about the model, such as which pipeline trained the model, the pipeline
configuration, etc. (see [Govenance](./6.md) for more information).

## Model Propagation from MCenter Server to Inference Pipelines
There are several ways that an inference pipeline can obtain the trained model
it will use for generating inferences:

- By setting it as the [Default Model for the pipeline](./4_3.md).
- By setting it manually using the [Set Model](./7_1.md#set_model) functionality.

<a name="propagation_policy"></a>
If the MLApp contains a training pipeline connected to the inference pipeline, the trained
model can also be set from the training pipeline depending on how the MLApp model propagation
policy is configured:

- **Always Propagate** indicates that every time a trained model is produced by the connected training
pipeline, MCenter should forward the model to the inference pipeline to be used when the inference pipeline
is next launched.

- **Manually Approve** indicates that trained models generated by the training pipeline should await
human review before being propagated to the inference pipelines.
See the [Review Models](./7_1.md#review_models) section for details.